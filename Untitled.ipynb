{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4552ae79-5f5f-431b-82fb-da98c0e1d660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7870\n",
      "\n",
      "Thanks for being a Gradio user! If you have questions or feedback, please join our Discord server and chat with us: https://discord.gg/feTf9x3ZSB\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/media/ashish/DATA/Python37/Projects/Retina-Blood-Vessel-Segmentation-Using-UNET-In-Pytorch/venv/lib/python3.8/site-packages/gradio/queueing.py\", line 456, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/media/ashish/DATA/Python37/Projects/Retina-Blood-Vessel-Segmentation-Using-UNET-In-Pytorch/venv/lib/python3.8/site-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/media/ashish/DATA/Python37/Projects/Retina-Blood-Vessel-Segmentation-Using-UNET-In-Pytorch/venv/lib/python3.8/site-packages/gradio/blocks.py\", line 1522, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/media/ashish/DATA/Python37/Projects/Retina-Blood-Vessel-Segmentation-Using-UNET-In-Pytorch/venv/lib/python3.8/site-packages/gradio/blocks.py\", line 1144, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/media/ashish/DATA/Python37/Projects/Retina-Blood-Vessel-Segmentation-Using-UNET-In-Pytorch/venv/lib/python3.8/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/media/ashish/DATA/Python37/Projects/Retina-Blood-Vessel-Segmentation-Using-UNET-In-Pytorch/venv/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/media/ashish/DATA/Python37/Projects/Retina-Blood-Vessel-Segmentation-Using-UNET-In-Pytorch/venv/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/media/ashish/DATA/Python37/Projects/Retina-Blood-Vessel-Segmentation-Using-UNET-In-Pytorch/venv/lib/python3.8/site-packages/gradio/utils.py\", line 674, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_151142/224990687.py\", line 81, in image_classifier\n",
      "    cat_images = np.concatenate(\n",
      "  File \"<__array_function__ internals>\", line 200, in concatenate\n",
      "ValueError: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 1 and the array at index 1 has size 512\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/ashish/DATA/Python37/Projects/Retina-Blood-Vessel-Segmentation-Using-UNET-In-Pytorch/venv/lib/python3.8/site-packages/gradio/queueing.py\", line 456, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/media/ashish/DATA/Python37/Projects/Retina-Blood-Vessel-Segmentation-Using-UNET-In-Pytorch/venv/lib/python3.8/site-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/media/ashish/DATA/Python37/Projects/Retina-Blood-Vessel-Segmentation-Using-UNET-In-Pytorch/venv/lib/python3.8/site-packages/gradio/blocks.py\", line 1522, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/media/ashish/DATA/Python37/Projects/Retina-Blood-Vessel-Segmentation-Using-UNET-In-Pytorch/venv/lib/python3.8/site-packages/gradio/blocks.py\", line 1144, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/media/ashish/DATA/Python37/Projects/Retina-Blood-Vessel-Segmentation-Using-UNET-In-Pytorch/venv/lib/python3.8/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/media/ashish/DATA/Python37/Projects/Retina-Blood-Vessel-Segmentation-Using-UNET-In-Pytorch/venv/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/media/ashish/DATA/Python37/Projects/Retina-Blood-Vessel-Segmentation-Using-UNET-In-Pytorch/venv/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/media/ashish/DATA/Python37/Projects/Retina-Blood-Vessel-Segmentation-Using-UNET-In-Pytorch/venv/lib/python3.8/site-packages/gradio/utils.py\", line 674, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_151142/224990687.py\", line 81, in image_classifier\n",
      "    cat_images = np.concatenate(\n",
      "  File \"<__array_function__ internals>\", line 200, in concatenate\n",
      "ValueError: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 1 and the array at index 1 has size 512\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/ashish/DATA/Python37/Projects/Retina-Blood-Vessel-Segmentation-Using-UNET-In-Pytorch/venv/lib/python3.8/site-packages/gradio/queueing.py\", line 501, in process_events\n",
      "    response = await self.call_prediction(awake_events, batch)\n",
      "  File \"/media/ashish/DATA/Python37/Projects/Retina-Blood-Vessel-Segmentation-Using-UNET-In-Pytorch/venv/lib/python3.8/site-packages/gradio/queueing.py\", line 465, in call_prediction\n",
      "    raise Exception(str(error) if show_error else None) from error\n",
      "Exception: None\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "from werkzeug.utils import secure_filename\n",
    "import cv2\n",
    "import torch\n",
    "from components.unet.unet_model import build_unet\n",
    "from torchvision import transforms\n",
    "\n",
    "def mask_parse(mask):\n",
    "    mask = np.expand_dims(mask, axis = -1)    ## (512, 512, 1)\n",
    "    mask = np.concatenate([mask, mask, mask], axis = -1)  ## (512, 512, 3)\n",
    "    return mask\n",
    "\n",
    "def image_classifier(inp):\n",
    "    # Reading the image into PIL format.\n",
    "        transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        # Add more preprocessing if needed\n",
    "    ])\n",
    "        pil_img = transform(inp).unsqueeze(0)\n",
    "\n",
    "        \n",
    "        # Converting it into opencv format to do some operations\n",
    "        opencvImage = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "\n",
    "        \"\"\" Hyperparameters \"\"\"\n",
    "        H = 512\n",
    "        W = 512\n",
    "        SIZE = (H, W)\n",
    "        CHECKPOINT_PATH = os.path.join(os.getcwd(), \"files\", \"checkpoint.pth\")\n",
    "\n",
    "\n",
    "        \"\"\" Load the Checkpoint \"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = build_unet()\n",
    "        model = model.to(device)\n",
    "        model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location = device))\n",
    "        \n",
    "\n",
    "        \"\"\"\" Reading the image \"\"\"\n",
    "       \n",
    "        x = cv2.resize(src = opencvImage, dsize = SIZE)\n",
    "        print(x.shape)\n",
    "        x = np.transpose(x, (2, 0, 1))  # (3, 512, 512)\n",
    "        x = x / 255.0\n",
    "        x = np.expand_dims(x, axis = 0)  # (1, 3, 512, 512)  Batch Size added.\n",
    "        x = x.astype(np.float32)\n",
    "        x = torch.from_numpy(x)\n",
    "        x = x.to(device)\n",
    "\n",
    "\n",
    "        \"\"\" Make Prediction \"\"\"\n",
    "        with torch.inference_mode():\n",
    "            y_pred = model(x)\n",
    "            y_pred = torch.sigmoid(y_pred)\n",
    "            y_pred = y_pred[0].cpu().numpy()  # (1, 512, 512)wwwwwwwwww\n",
    "            y_pred = np.squeeze(y_pred, axis = 0)  # (512, 512)\n",
    "            y_pred = y_pred > 0.5\n",
    "            y_pred = np.array(y_pred, dtype = np.uint8)\n",
    "\n",
    "\n",
    "        \"\"\" Saving masks \"\"\"\n",
    "        y_pred = mask_parse(y_pred)\n",
    "        y_pred = y_pred * 255\n",
    "        line = np.ones((SIZE[1], 10, 3)) * 128\n",
    "\n",
    "        opencvImage = cv2.putText(img = opencvImage, text = \"original image\",\n",
    "                            org = (0, 30), fontFace = cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
    "                            fontScale = 1, color = [0, 0, 255], thickness = 2)\n",
    "\n",
    "        y_pred = cv2.putText(img = y_pred, text = \"predicted mask\",\n",
    "                            org = (0, 30), fontFace = cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
    "                            fontScale = 1, color = [0, 0, 255], thickness = 2)\n",
    "\n",
    "        \n",
    "        cat_images = np.concatenate(\n",
    "            [opencvImage, line, y_pred], axis = 1\n",
    "        )\n",
    "\n",
    "        filename = filename.split(\".\")[0]\n",
    "        resulted_filename = f\"{filename}_result.png\"\n",
    "        \n",
    "        resulted_filename_path = os.path.join(os.getcwd(), \"static\", \"uploads\", resulted_filename)\n",
    "        cv2.imwrite(resulted_filename_path, cat_images)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # # Reading the Resulted Image into PIL format.\n",
    "        # pil_img = Image.open(os.path.join(os.getcwd(), \"static\", \"uploads\"), resulted_filename)\n",
    "        \n",
    "        # data = io.BytesIO()\n",
    "        # pil_img.save(data, \"png\")\n",
    "        \n",
    "        # encode_img_data = base64.b64encode(data.getvalue())\n",
    "        return os.path.join(resulted_filename_path, cat_images)\n",
    "\n",
    "\n",
    "\n",
    "gr.Interface(fn=image_classifier,\n",
    "             inputs=gr.Image(type=\"pil\"),\n",
    "             outputs=\"image\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d683dfe-d539-4976-90e8-3f65d1e1fccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f69a12-7509-4c95-b407-241e4fa8e17b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0940b4a9-e368-4852-a03e-979d21340d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fc2b4cf-812e-4513-a4ae-dc5de46df9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "def sepia(input_img):\n",
    "    sepia_filter = np.array([\n",
    "        [0.393, 0.769, 0.189], \n",
    "        [0.349, 0.686, 0.168], \n",
    "        [0.272, 0.534, 0.131]\n",
    "    ])\n",
    "    sepia_img = input_img.dot(sepia_filter.T)\n",
    "    sepia_img /= sepia_img.max()\n",
    "    return sepia_img\n",
    "\n",
    "demo = gr.Interface(sepia, gr.Image(), \"image\")\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98c2d13-2a72-459c-bab1-014bb8291e12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
